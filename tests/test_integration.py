#!/usr/bin/env python3
"""çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ - ã‚¿ã‚¹ã‚¯8.1 ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""

import json
import os
import subprocess
import tempfile
import time
from pathlib import Path

import pytest
import requests


API_URL = "http://localhost:18081"


class TestIntegration:
    """çµ±åˆãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""

    @classmethod
    def setup_class(cls):
        """ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
        print("\n=== çµ±åˆãƒ†ã‚¹ãƒˆé–‹å§‹ ===\n")

    def test_cli_basic_functionality(self):
        """CLIã®åŸºæœ¬æ©Ÿèƒ½ãŒå‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        # ãƒ†ã‚¹ãƒˆç”¨JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        content = '''{"inference1": "test1", "inference2": "test2"}
{"inference1": "test3", "inference2": "test4"}
{"inference1": "test5", "inference2": "test6"}
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
            f.write(content)
            temp_file = f.name

        try:
            # CLIã§ã‚¹ã‚³ã‚¢è¨ˆç®—ã‚’å®Ÿè¡Œ
            result = subprocess.run(
                ['uv', 'run', 'python', '-m', 'src', temp_file, '--type', 'score'],
                capture_output=True,
                text=True
            )

            assert result.returncode == 0, f"CLIã‚¨ãƒ©ãƒ¼: {result.stderr}"

            # å‡ºåŠ›ã‚’JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
            output = json.loads(result.stdout)

            # å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®ç¢ºèª
            assert 'score' in output or 'overall_similarity' in output
            assert 'total_lines' in output or 'statistics' in output

            print("âœ… CLIåŸºæœ¬æ©Ÿèƒ½: æ­£å¸¸å‹•ä½œ")

        finally:
            os.unlink(temp_file)

    def test_api_endpoints_availability(self):
        """ã™ã¹ã¦ã®APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        endpoints = [
            ('GET', '/'),
            ('GET', '/health'),
            ('GET', '/ui'),
            ('GET', '/metrics')
        ]

        for method, endpoint in endpoints:
            response = requests.request(method, f"{API_URL}{endpoint}")
            assert response.status_code == 200, f"{endpoint} ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
            print(f"âœ… {method} {endpoint}: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {response.status_code}")

    def test_upload_and_download_flow(self):
        """ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰â†’å‡¦ç†â†’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã®ä¸€é€£ã®ãƒ•ãƒ­ãƒ¼ã‚’ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆç”¨JSONLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        content = '''{"inference1": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ1", "inference2": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ2"}
{"inference1": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ3", "inference2": "ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ4"}
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
            f.write(content)
            temp_file = f.name

        try:
            # 1. ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            with open(temp_file, 'rb') as f:
                files = {'file': ('test.jsonl', f, 'application/jsonl')}
                data = {'type': 'file', 'gpu': 'false'}

                upload_response = requests.post(
                    f"{API_URL}/upload",
                    files=files,
                    data=data
                )

            assert upload_response.status_code == 200, f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {upload_response.text}"
            result = upload_response.json()

            # 2. çµæœã®ç¢ºèª
            assert isinstance(result, list), "fileå½¢å¼ã®çµæœã¯ãƒªã‚¹ãƒˆã§ã‚ã‚‹ã¹ã"
            assert len(result) > 0, "çµæœãŒç©ºã§ã™"

            # 3. CSVå¤‰æ›ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚µã‚¤ãƒ‰ï¼‰ã®ãƒ†ã‚¹ãƒˆ
            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ†ã‚¹ãƒˆ
            csv_response = requests.post(
                f"{API_URL}/download/csv",
                json=result,
                params={'type': 'file'}
            )

            assert csv_response.status_code == 200, "CSVå¤‰æ›å¤±æ•—"
            content_type = csv_response.headers.get('content-type', '')
            assert content_type.startswith('text/csv'), f"CSV content-typeãŒä¸æ­£: {content_type}"

            print("âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰â†’å‡¦ç†â†’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ãƒ­ãƒ¼: æ­£å¸¸å®Œäº†")

        finally:
            os.unlink(temp_file)

    def test_error_handling_integration(self):
        """ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # 1. ç„¡åŠ¹ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write("Not a JSONL file")
            temp_file = f.name

        try:
            with open(temp_file, 'rb') as f:
                files = {'file': ('test.txt', f, 'text/plain')}
                response = requests.post(f"{API_URL}/upload", files=files)

            assert response.status_code == 400
            error_detail = response.json()['detail']
            assert 'error_id' in error_detail
            assert error_detail['error_id'].startswith('ERR-')
            print(f"âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°: {error_detail['error_id']} ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸ")

        finally:
            os.unlink(temp_file)

    def test_metrics_collection(self):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ"""
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹
        response = requests.get(f"{API_URL}/metrics")
        assert response.status_code == 200

        metrics = response.json()
        assert 'upload_metrics' in metrics
        assert 'timestamp' in metrics

        upload_metrics = metrics['upload_metrics']

        if 'total_uploads' in upload_metrics and upload_metrics['total_uploads'] > 0:
            assert 'success_rate' in upload_metrics
            assert 'average_processing_time' in upload_metrics

            print(f"âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†:")
            print(f"  - ç·ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ•°: {upload_metrics['total_uploads']}")
            print(f"  - æˆåŠŸç‡: {upload_metrics['success_rate']}%")
            print(f"  - å¹³å‡å‡¦ç†æ™‚é–“: {upload_metrics['average_processing_time']}ç§’")
        else:
            print("âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†: ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆæ­£å¸¸å‹•ä½œ")

    def test_parallel_processing_capability(self):
        """ä¸¦åˆ—å‡¦ç†èƒ½åŠ›ã®ãƒ†ã‚¹ãƒˆï¼ˆè² è·ãƒ†ã‚¹ãƒˆï¼‰"""
        import concurrent.futures

        def upload_file():
            """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"""
            content = '''{"inference1": "parallel test 1", "inference2": "parallel test 2"}
{"inference1": "parallel test 3", "inference2": "parallel test 4"}
'''
            with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=True) as f:
                f.write(content)
                f.flush()

                with open(f.name, 'rb') as file:
                    files = {'file': ('parallel.jsonl', file, 'application/jsonl')}
                    data = {'type': 'score', 'gpu': 'false'}
                    response = requests.post(f"{API_URL}/upload", files=files, data=data)

                return response.status_code == 200

        # 5ã¤ã®ä¸¦åˆ—ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
            start_time = time.time()
            futures = [executor.submit(upload_file) for _ in range(5)]
            results = [f.result() for f in concurrent.futures.as_completed(futures)]
            elapsed_time = time.time() - start_time

        success_count = sum(results)
        print(f"âœ… ä¸¦åˆ—å‡¦ç†ãƒ†ã‚¹ãƒˆ:")
        print(f"  - æˆåŠŸ: {success_count}/5")
        print(f"  - å‡¦ç†æ™‚é–“: {elapsed_time:.2f}ç§’")

        assert success_count >= 3, "ä¸¦åˆ—å‡¦ç†ã§å°‘ãªãã¨ã‚‚3ã¤ã¯æˆåŠŸã™ã‚‹ã¹ã"

    def test_cli_and_api_consistency(self):
        """CLIã¨APIã®çµæœãŒä¸€è‡´ã™ã‚‹ã“ã¨ã‚’ç¢ºèª"""
        # ãƒ†ã‚¹ãƒˆç”¨JSONLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        content = '''{"inference1": "consistency test 1", "inference2": "consistency test 2"}
{"inference1": "consistency test 3", "inference2": "consistency test 4"}
'''
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False) as f:
            f.write(content)
            temp_file = f.name

        try:
            # 1. CLIå®Ÿè¡Œ
            cli_result = subprocess.run(
                ['uv', 'run', 'python', '-m', 'src', temp_file, '--type', 'score'],
                capture_output=True,
                text=True
            )
            cli_output = json.loads(cli_result.stdout) if cli_result.returncode == 0 else None

            # 2. APIå®Ÿè¡Œ
            with open(temp_file, 'rb') as f:
                files = {'file': ('test.jsonl', f, 'application/jsonl')}
                data = {'type': 'score', 'gpu': 'false'}
                api_response = requests.post(f"{API_URL}/upload", files=files, data=data)
            api_output = api_response.json() if api_response.status_code == 200 else None

            # 3. çµæœã®æ¯”è¼ƒï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é™¤ãï¼‰
            if cli_output and api_output:
                # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
                if '_metadata' in api_output:
                    del api_output['_metadata']

                # åŸºæœ¬çš„ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®å­˜åœ¨ç¢ºèª
                for key in ['score', 'overall_similarity']:
                    if key in cli_output:
                        assert key in api_output or 'overall_similarity' in api_output or 'score' in api_output
                        break

                print("âœ… CLI/APIä¸€è²«æ€§: åŸºæœ¬çš„ãªäº’æ›æ€§ç¢ºèª")
            else:
                pytest.skip("CLIã¾ãŸã¯APIã®å®Ÿè¡Œã«å¤±æ•—")

        finally:
            os.unlink(temp_file)

    def test_log_files_creation(self):
        """ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãä½œæˆã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª"""
        log_dir = Path("/tmp/json_compare/logs")

        if log_dir.exists():
            log_files = list(log_dir.glob("*.log"))
            expected_logs = ["access.log", "error.log", "metrics.log"]

            found_logs = [f.name for f in log_files]

            for expected in expected_logs:
                if expected in found_logs:
                    log_file = log_dir / expected
                    assert log_file.exists(), f"{expected} ãŒå­˜åœ¨ã—ã¾ã›ã‚“"
                    assert log_file.stat().st_size >= 0, f"{expected} ãŒç©ºã§ã™"

            print(f"âœ… ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ:")
            print(f"  - ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {log_dir}")
            print(f"  - å­˜åœ¨ã™ã‚‹ãƒ­ã‚°: {found_logs}")
        else:
            print("âš ï¸ ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒã¾ã ä½œæˆã•ã‚Œã¦ã„ã¾ã›ã‚“")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•° - çµ±åˆãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ"""
    print("\n" + "="*60)
    print("JSON Compare çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ (ã‚¿ã‚¹ã‚¯8.1)")
    print("="*60 + "\n")

    # APIã‚µãƒ¼ãƒãƒ¼ã®èµ·å‹•ç¢ºèª
    try:
        response = requests.get(f"{API_URL}/health", timeout=2)
        if response.status_code != 200:
            print("âŒ APIã‚µãƒ¼ãƒãƒ¼ãŒèµ·å‹•ã—ã¦ã„ã¾ã›ã‚“")
            print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§APIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
            print("  uv run uvicorn src.api:app --host 0.0.0.0 --port 18081")
            return 1
    except requests.exceptions.RequestException:
        print("âŒ APIã‚µãƒ¼ãƒãƒ¼ã«æ¥ç¶šã§ãã¾ã›ã‚“")
        print("ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§APIã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã¦ãã ã•ã„:")
        print("  uv run uvicorn src.api:app --host 0.0.0.0 --port 18081")
        return 1

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    test_suite = TestIntegration()

    tests = [
        ("CLIåŸºæœ¬æ©Ÿèƒ½", test_suite.test_cli_basic_functionality),
        ("APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆå¯ç”¨æ€§", test_suite.test_api_endpoints_availability),
        ("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰â†’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ãƒ­ãƒ¼", test_suite.test_upload_and_download_flow),
        ("ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°çµ±åˆ", test_suite.test_error_handling_integration),
        ("ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†", test_suite.test_metrics_collection),
        ("ä¸¦åˆ—å‡¦ç†èƒ½åŠ›", test_suite.test_parallel_processing_capability),
        ("CLI/APIä¸€è²«æ€§", test_suite.test_cli_and_api_consistency),
        ("ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ", test_suite.test_log_files_creation)
    ]

    failed_tests = []

    for test_name, test_func in tests:
        try:
            print(f"\n[å®Ÿè¡Œä¸­] {test_name}...")
            test_func()
        except Exception as e:
            print(f"âŒ {test_name}: {e}")
            failed_tests.append(test_name)

    # çµæœã‚µãƒãƒªãƒ¼
    print("\n" + "="*60)
    print("çµ±åˆãƒ†ã‚¹ãƒˆçµæœã‚µãƒãƒªãƒ¼")
    print("="*60)
    print(f"âœ… æˆåŠŸ: {len(tests) - len(failed_tests)}/{len(tests)}")

    if failed_tests:
        print(f"âŒ å¤±æ•—: {', '.join(failed_tests)}")
        return 1
    else:
        print("\nğŸ‰ ã™ã¹ã¦ã®çµ±åˆãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã¾ã—ãŸï¼")
        print("\nè¦ä»¶ç¢ºèª:")
        print("âœ… 7.3: ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®çµ±åˆç¢ºèªå®Œäº†")
        print("âœ… æ—¢å­˜CLIæ©Ÿèƒ½ã¸ã®å½±éŸ¿ãªã—")
        print("âœ… Web UIã¨CLIä¸¡æ–¹ã§ã®å‹•ä½œç¢ºèªå®Œäº†")
        print("âœ… å…¨ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆé–“ã®ç›¸äº’ä½œç”¨ç¢ºèª")
        print("âœ… ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®ç¶²ç¾…æ€§ç¢ºèª")
        return 0


if __name__ == "__main__":
    exit(main())